{"posts":[{"title":"Time Series Basics","content":"Definition Stochastic process: A stochastic process is a collection of random variables {Xt}t∈T\\lbrace X_t\\rbrace_{t \\in T}{Xt​}t∈T​ indexed by a set TTT , i.e. t∈Tt \\in Tt∈T defined on a probability space (Ω,F,P)( \\Omega, \\mathcal {F},P)(Ω,F,P). (Not necessarily independent) If TTT consists of the integers (or a subset), the process is called a Discrete Time Stochastic Process. If TTT consists of the real numbers (or a subset), the process is called Continuous Time Stochastic Process. Realization: A realization of {Xt}\\lbrace X_t\\rbrace{Xt​} is the outcome {xt}t∈T\\lbrace x_t\\rbrace_{t \\in T}{xt​}t∈T​ = {Xt(ω)}t∈T\\lbrace X_t(\\omega)\\rbrace_{t \\in T}{Xt​(ω)}t∈T​ for some ω∈Ω\\omega \\in \\Omegaω∈Ω. Time Series: 'We use the term time series whether we are referring generically to the process or to a particular realization and make no notational distinction between the two concepts.'[1] Describing a stochastic process A simpler, more useful way of describing a stochastic process is to give the moments of the process, particularly the ﬁrst and second moments that are called the mean and autocovariance function (acv.f.), respectively.[2] Mean The mean function μ(t)\\mu(t)μ(t) is deﬁned for all ttt by μ(t)=E[X(t)]\\mu(t) = E[X(t)] μ(t)=E[X(t)] Autocovariance The variance function σ2(t)\\sigma^2(t)σ2(t) is deﬁned for all ttt by σ2(t)=Var[X(t)]=E[(X(t)−μ(t))2]\\sigma^2(t)=Var[X(t)]=E[(X(t)-\\mu(t))^2] σ2(t)=Var[X(t)]=E[(X(t)−μ(t))2] The variance function alone is not enough to specify the second moments of a sequence of random variables. More generally, we deﬁne the acv.f. γ(t1,t2)\\gamma(t_1,t_2)γ(t1​,t2​) to be the covariance of X(t1)X(t_1)X(t1​) with X(t2)X(t_2)X(t2​), namely γ(t1,t2)=E{[X(t1)−μ(t1)][X(t2)−μ(t2)]}\\gamma(t_1,t_2)=E\\lbrace [X(t_1)-\\mu(t_1)][X(t_2)-\\mu(t_2)]\\rbrace γ(t1​,t2​)=E{[X(t1​)−μ(t1​)][X(t2​)−μ(t2​)]} The variance function is a special case of the acv.f. when t1=t2t_1 = t_2t1​=t2​. The autocorrelation function is defined as ρ(t1,t2)=γ(t1,t2){γ(t1,t1)γ(t2,t2)}1/2\\rho(t_1,t_2)=\\frac{\\gamma(t_1,t_2)}{\\lbrace \\gamma(t_1,t_1)\\gamma(t_2,t_2)\\rbrace ^{1/2}} ρ(t1​,t2​)={γ(t1​,t1​)γ(t2​,t2​)}1/2γ(t1​,t2​)​ Some classes of stochastic processes Stationary Processes An important class of stochastic processes are those that are stationary. Intuitive Point of View Broadly speaking a time series is said to be stationary if there is no systematic change in mean (no trend), if there is no systematic change in variance and if strictly periodic variations have been removed. In other words, the properties of one section of the data are much like those of any other section.[2:1] Mathematical Deﬁnition A time series is said to be strictly stationary if the joint distribution of X(t1),...,X(tk)X(t_1), . . . , X(t_k)X(t1​),...,X(tk​) is the same as the joint distribution of X(t1+τ),...,X(tk+τ)X(t_{1+\\tau}), . . . , X(t_{k+\\tau})X(t1+τ​),...,X(tk+τ​) for all t1,...,tk,τt_1, . . . , t_k, \\taut1​,...,tk​,τ. second-order stationary (or weakly stationary) if the mean is constant and the covariance function γ(s,t)\\gamma(s, t)γ(s,t) depends only on the lag ∣t−s∣(τ)|t − s|(\\tau)∣t−s∣(τ). Thus, the autocovariance function can be written as γ(t,t+τ)=γ(0,τ)=γ(0,−τ)≡γ∣τ∣=γτ\\gamma(t,t+\\tau)=\\gamma(0,\\tau)=\\gamma(0,-\\tau)\\equiv \\gamma_{|\\tau|}=\\gamma_\\tau γ(t,t+τ)=γ(0,τ)=γ(0,−τ)≡γ∣τ∣​=γτ​ τ∈Z\\tau \\in \\mathcal {Z}τ∈Z. The autocorrelation function can be written as ρ(t,t+τ)≡ρ∣τ∣=ρτ\\rho(t,t+\\tau)\\equiv \\rho_{|\\tau|}=\\rho_\\tau ρ(t,t+τ)≡ρ∣τ∣​=ρτ​ In the stationary case the covariance and correlation functions are symmetric around τ=0\\tau = 0τ=0. Purely Random Processes (White Noise) A stochastic process Xt{X_t}Xt​ is called white noise if its elements are uncorrelated, mean E(Xt)=0E(X_t) = 0E(Xt​)=0 and variance Var(Xt)=σ2Var(X_t) = \\sigma^2Var(Xt​)=σ2. γ(k)=Cov(Xt,Xt+k)={σX2,k=00,k=±1,±2,...\\gamma(k)=Cov(X_t,X_{t+k})=\\begin{cases}\\sigma^2_X, &amp;k=0 \\cr 0, &amp;k=\\pm1,\\pm2,... \\end{cases} γ(k)=Cov(Xt​,Xt+k​)={σX2​,0,​k=0k=±1,±2,...​ ρ(k)=Cor(Xt,Xt+k)={1,k=00,k=±1,±2,...\\rho(k)=Cor(X_t,X_{t+k})=\\begin{cases}1, &amp;k=0 \\cr 0, &amp;k=\\pm1,\\pm2,... \\end{cases} ρ(k)=Cor(Xt​,Xt+k​)={1,0,​k=0k=±1,±2,...​ If in addition the XtX_tXt​ are normally distributed, then we have Gaussian white noise Xt∼iidN(0,σ2)X_t \\overset{iid}{\\sim}\\mathcal {N} (0, \\sigma^2)Xt​∼iidN(0,σ2). As the mean and acv.f. do not depend on time, the process is second-order stationary. In practice, if all sample ac.f.’s of a series are close to zero, then the series is considered as a realization of a purely random process. Robert H. Shumway, David S. Stoﬀer - Time Series Analysis and its Applications with R Examples ↩︎ The Analysis of Time Series - an Introduction, Chatﬁeld ↩︎ ↩︎ ","link":"https://lyzxh.github.io/post/Time-Series-Basics"},{"title":"闲谈感恩的表达","content":"就像说“谢谢”是为了展示自己的礼貌和教养一样，表达感恩也是为了展示自己是一个知恩图报值得交往的人。因为出发点是利己的，所以很多时候我们的表现都十分敷衍，仿佛不屑掩饰或者不知道如何掩饰利己的心一样。平淡而没有声调起伏的“谢谢”，或是有力度却过于浮夸的排比式“感谢xx的xx”，都是例证。 这话很是难听，却也是事实。破局手段有二，一是真诚，二是技巧。 有的人温暖如小太阳，感谢和感恩都是来自心底，他们的表达没有多少矫饰，或许磕绊，或许语无伦次，但会让人感受到那颗真心。当然若是加上技巧，感染力自然更上层楼。 有的人共情能力低，同理心欠缺，他的感谢和感恩往往难断真假。你让他叩问自己，是真的感谢别人吗，还是拘于礼节和教养而已呢？或许他自己都说不清楚。似这样冷心冷肺的人，要让对方真的感知到你的感谢和感恩，是需要技巧的。感恩对方，举实例，列细节，万不可虚头巴脑地堆砌好词，那叫溜须拍马，与感恩差得远呢。对同理心欠缺的人来说，一分真心，九分技巧，亦是很不错的感恩表达了。 若是一分真心也没有，那便是另一个话题了。都说人生如戏，全靠演技。你一点都不觉得感谢对方是值得的，却还得表现得真心实意，那不就只能演了嘛。这就难了。难点不在背着心感谢别人，难点在感谢的事例。或许你一贯认为对方是傻逼，是烂泥扶不上墙，自己是不屑与之为伍的，所以你很难想起什么能用来感谢的细节。俗话说“屎都能夸出花来”，这种情况下欠缺的便是这“屎里找花”的观察力了。 这也不容易。不过我仔细想想，倒是有一个小妙招可以试试。比如你要演出感谢一个你心里已经亲封为史上第一大傻逼的同事，要用具体的事例来佐证。你就回忆一下，平常生活中，有没有和你的小伙伴八卦吐槽过他的事儿，然后使出“闭眼瞎吹反着夸”大招，当初是怎么吐槽的，现在就反过来怎么吹。比如： 当初的吐槽：“你看他做的这个ppt，可能小学五年级的学生都比他会配色” 闭眼瞎感谢：“其实不是我一个人的功劳，是我公司团队合作的力量。比如我身边这位同事吧，没有他初稿PPT的大胆配色和精心设计给我们带来灵感，根本不可能有现在这个效果的。” 不过要是你连对方做过什么事都不知道，那就不太好操作了。所以，日常生活中还是多观察，默默积累素材。 感恩是个技术活，得多练，练到深入骨髓，练到条件反射，或许到那时候，自己也分不清自己的感恩几分真几分假了。 ","link":"https://lyzxh.github.io/post/闲谈感恩的表达"}]}